---
title: "Article graph"
author: "John Coene"
date: 2019-07-09T00:00:00+02:00
categories: ["R"]
tags: ["R Markdown", "weforum"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
library(gensimr)
library(webhoser)

load("./hk_articles.RData")

lda_model <- load_word2vec("./hk_articles.model")

# specify python3
os <- Sys.info()["sysname"]
if(os == "Darwin") 
  reticulate::use_python("/usr/local/bin/python3", required = TRUE)
else
  reticulate::use_python("/usr/bin/python3", required = TRUE)
```

r-news' first blog post ðŸŽ‰

In this blog post we gather coverage on [anti-extradition bill protests](https://en.wikipedia.org/wiki/2019_Hong_Kong_anti-extradition_bill_protests) taking place in Honk Kong to build a Latent Dirichlet Allocation model.

First we collect 1,000 articles on the protests using [webhoser](https://webhoser.news-r.org/).

```r
library(webhoser)

hk_articles <- wh_news(
    q = '"Hong Kong" AND ("protest" OR "protests") is_first:true language:english site_type:news',
    ts = (Sys.time() - (30 * 24 * 60 * 60))
  ) %>% 
  wh_paginate(9) %>% #Â 9 pages + initial query
  wh_collect()
```

This returns 10 pages of results (an initial query and additional 9 pages). We can clean up the documents; we

```r
library(gensimr)

# preprocess
corpus <- preprocess(hk_articles$text)
dictionary <- corpora_dictionary(corpus)
corpora <- doc2bow(dictionary, corpus)

# matrix market
corpus_mm <- serialize_mmcorpus(corpora)

# tf-idf
tfidf_model <- model_tfidf(corpus_mm)
corpus_tfidf <- wrap(tfidf_model, corpora)

# build model
lda_model <- model_lda(
  corpus_tfidf,
  id2word = dictionary, 
  num_topics = 50L
)

vis <- prepare_ldavis(lda_model, corpora, dictionary)
```
