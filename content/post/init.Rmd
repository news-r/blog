---
title: "Article graph"
author: "John Coene"
date: 2019-07-09T00:00:00+02:00
categories: ["R"]
tags: ["R Markdown", "weforum"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
library(gensimr)
library(webhoser)

load("./hk_articles.RData")
```

r-news' first blog post ðŸŽ‰

In this blog post we gather coverage on [anti-extradition bill protests](https://en.wikipedia.org/wiki/2019_Hong_Kong_anti-extradition_bill_protests) ...

First we collect 7,700 articles on the protests using [webhoser](https://webhoser.news-r.org/) (these were all the "high confidence" articles webhose.io could find).

```r
library(webhoser)

hk_articles <- wh_news(
    q = '"Hong Kong" AND ("protest" OR "protests") is_first:true language:english site_type:news',
    ts = (Sys.time() - (30 * 24 * 60 * 60)),
    accuracy = "high",
    highlight = TRUE
  ) %>% 
  wh_paginate(99) %>% #Â 99 pages + initial query = 100 pages
  wh_collect()
```

This returns 10 pages of results (an initial query and additional 9 pages). We can clean up the documents; we

```{r}
library(word2vec.r)

setup_word2vec()

# VERY basic preprocessing
# to lower case all words
# remove punctuation
# collapse to a single string
preprocess <- function(txt){
  txt <- tolower(txt)
  txt <- gsub("[^[:alnum:][:space:]]", " ", txt)
  txt <- gsub("[[:space:]]+", " ", txt)
  trimws(txt)
}

articles <- preprocess(hk_articles$text)

model_path <- word2vec(articles, alpha = 0.02)
model <- word_vectors(model_path)
```
