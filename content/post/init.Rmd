---
title: "Article graph"
author: "John Coene"
date: 2019-07-09T00:00:00+02:00
categories: ["R"]
tags: ["R Markdown", "weforum"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
library(gensimr)
library(webhoser)

load("./hk_articles.RData")

word2vec <- load_word2vec("./hk_articles.model")

# specify python3
os <- Sys.info()["sysname"]
if(os == "Darwin") 
  reticulate::use_python("/usr/local/bin/python3", required = TRUE)
else
  reticulate::use_python("/usr/bin/python3", required = TRUE)
```

r-news' first blog post ðŸŽ‰

In this blog post we gather coverage on [anti-extradition bill protests](https://en.wikipedia.org/wiki/2019_Hong_Kong_anti-extradition_bill_protests) taking place in Honk Kong to build word embeddings using Word2Vec which we then reduce with t-sne.

First we collect 1,000 articles on the protests using [webhoser](https://webhoser.news-r.org/).

```r
library(webhoser)

hk_articles <- wh_news(
    q = '"Hong Kong" AND ("protest" OR "protests") is_first:true language:english site_type:news',
    ts = (Sys.time() - (30 * 24 * 60 * 60))
  ) %>% 
  wh_paginate(9) %>% #Â 9 pages + initial query
  wh_collect()
```

This returns 10 pages of results (an initial query and additional 9 pages). We can clean up the documents; we

```r
library(gensimr)

corpus <- preprocess(hk_articles$text)

# create model
word2vec <- model_word2vec(
  size = 300L, 
  window = 4L,  
  workers = 2L,
  negative = 15L,
  min_count = 20L,
  ns_exponent = 0.75
)
word2vec$build_vocab(corpus) 
word2vec$init_sims(replace = TRUE)
word2vec$save("./hk_articles.model") # save

words <- word2vec$wv$index2entity %>% 
  reticulate::py_to_r() %>% 
  tibble::tibble(
    word = .
  )
vectors <- wrap(word2vec, word2vec$wv$vocab) # extrac vectors

# make matrix
matrix <- reticulate::py_to_r(vectors)

tsne <- Rtsne::Rtsne(matrix) # Run t-sne
tsne$Y %>% 
  tibble::as_tibble() %>% 
  purrr::set_names(c("x", "y")) %>%
  dplyr::bind_cols(words) %>%  
  g2r::g2(g2r::asp(x, y, tooltip = word)) %>% 
  g2r::fig_point()
```
